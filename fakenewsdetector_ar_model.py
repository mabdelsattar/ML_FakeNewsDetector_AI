# -*- coding: utf-8 -*-
"""FakeNewsDetector_Ar_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_YK4HE7mfyybGFMoG9mBkJHSdNVLN3lY
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("murtadhayaseen/arabic-fake-news-dataset-afnd")

print("Path to dataset files:", path)

import os
import json
import pandas as pd

# Paths
base_dir = "/kaggle/input/arabic-fake-news-dataset-afnd/AFND/Dataset"
sources_json_path = "/kaggle/input/arabic-fake-news-dataset-afnd/AFND/sources.json"

# Load source → label mapping
with open(sources_json_path, "r", encoding="utf-8") as f:
    source_to_label = json.load(f)  # Direct dictionary

# Prepare output containers
real_ar, fake_ar, undecided_ar = [], [], []
total_articles = 0
labeled_articles = 0

# Loop through all source folders
for folder in sorted(os.listdir(base_dir)):
    folder_path = os.path.join(base_dir, folder)
    json_path = os.path.join(folder_path, "scraped_articles.json")

    if not os.path.isfile(json_path):
        continue

    # Get label for this source folder
    label = source_to_label.get(folder, "").strip().lower()
    if label not in ["credible", "not credible", "undecided"]:
        continue  # Skip sources without valid label

    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        articles = data.get("articles", [])

        for article in articles:
            total_articles += 1
            text = article.get("text", "").strip()
            if not text:
                continue

            labeled_articles += 1
            row = {
                "source": folder,
                "title": article.get("title", "").strip(),
                "text": text,
                "date": article.get("date", article.get("published date", "")).strip(),
                "label": label
            }

            # Classify
            if label == "credible":
                real_ar.append(row)
            elif label == "not credible":
                fake_ar.append(row)
            elif label == "undecided":
                undecided_ar.append(row)

    except Exception as e:
        print(f" Error reading {json_path}: {e}")

# Export to CSV
pd.DataFrame(real_ar).to_csv("real_ar.csv", index=False)
pd.DataFrame(fake_ar).to_csv("fake_ar.csv", index=False)
pd.DataFrame(undecided_ar).to_csv("undecided_ar.csv", index=False)

# Summary
print("\nDone! Final stats:")
print(f"Total articles scanned: {total_articles}")
print(f"Labeled articles processed: {labeled_articles}")
print(f"real_ar.csv: {len(real_ar)}")
print(f"fake_ar.csv: {len(fake_ar)}")
print(f"undecided_ar.csv: {len(undecided_ar)}")

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.feature_extraction.text import TfidfVectorizer


arabic_stopwords = [
      "في", "من", "على", "إلى", "و", "عن", "أن", "إن", "كان", "هذه", "هذا", "ما", "لا", "لم", "لن", "قد",
      "هل", "هو", "هي", "ثم", "ذلك", "كل", "أو", "أي", "أين", "كيف", "لماذا", "متى", "ب", "ل", "حتى", "كما"
  ]

# Download latest version
fake_df = pd.read_csv('/content/fake_ar.csv')
real_df = pd.read_csv('/content/real_ar.csv')

fake_df['label'] = 0
real_df['label'] = 1
fake_df.shape

df = pd.concat([fake_df, real_df], ignore_index=True)
df['text'] = df['title'].fillna('') + ' ' + df['text'].fillna('')
df = df[['text', 'label']]
x = df['text']
y = df['label']
y.head()
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)
# Convert text to numeric features
vectorizer = TfidfVectorizer(stop_words=arabic_stopwords, max_df=0.7)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

model = LogisticRegression()
model.fit(X_train_tfidf, y_train)

# Predict
y_pred = model.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
print("Model Accuracy:", accuracy)

# def predict_news(text):
#       print(text)
#       text_vector = vectorizer.transform([text])  # transform to numeric
#       prediction = model.predict(text_vector)#X_test_tfidf[1]
#       return "أخبار كاذبة" if prediction[0] == 0 else "اخبار حقيقية"

# while True:
#     user_input = str(input(":أدخل مقالة إخبارية(أو اكتب 'خروج')"))
#     if user_input == 'خروج':
#         break
#     print("Prediction:", predict_news(user_input))

import joblib  # to save and load model
from google.colab import files

joblib.dump(model, 'ar_model.pkl')
files.download('ar_model.pkl')